\section{User Validation}

Usability is understood as the extent to which a product can be used by specified users to achieve specified goals with effectiveness, efficiency, and satisfaction in a specified context of use. 

This is at the use-case level and will be an iterative process throughout the project. Initially testing the common prototype, and later on enhanced with the specific customised dashboards. 

This evaluation involves end users in a near-life environment, in lab and field trials. It will engage professional users, such as monitors, editors, journalists, to test the use case in a (simulated) working environment. The question here is: ``Is the \SUMMA platform fit for purpose?''  The user evaluation is primarily aimed at testing usability to find out whether our intended users are happy with what \SUMMA can do for them. 

User evaluation includes summative evaluation and leads to formal feedback results to be analysed and visualised. User feedback will be collected digitally, for instance by means of an online form, with automated analysis and visualisation. This will gather feedback on ease of use, usefulness and user satisfaction. In addition to regular feedback cycles, special events such as user days, workshops, etc. will be organised to introduce the system to other user groups. Feedback is provided directly to the technical partner concerned or the entire consortium, fed into the JIRA system, and recorded in the wiki.

Usability tests are carried out with an initially small test user group. It will gradually be expanded and additional test users from the target group are involved at both user partner sites.

Since Deutsche Welle and BBC have different use cases and target groups, the evaluation at this level will be carried out separately, and the feedback mechanism will be coordinated. Deutsche Welle will primarily engage editorial teams at different levels; the BBC will involve monitoring journalists and other editorial staff. Each user partner will be responsible for preparing the test material for their test users, and for scheduling and carrying out their test sessions.

\subsection{External Monitoring Use Case}
Monitoring Journalists will in the main be using \SUMMA as a tool to point it to important events, entities and trends in the ingested media occurrences. Perfection in the ASR, machine translation and summarisation results will not be the focus in this particular use case. Results in spotless grammar will not be required. So, for example, in the majority of cases, the inaccurate rendering of a plural as a singular or vice versa will not affect the usefulness of the system. 
More important, however, is accuracy of substance. The regular omission (or spurious addition) of ‘not’ would be a fairly serious problem. Therefore the user evaluation of Use Case 1 will be focusing more on the output quality of the overall platform rather than comparing machine translation with human translation or ASR with human transcription.
\medskip

\textbf{User Evaluation Approach:} 

\textbf{Phase 1:} usability testing during the development of the initial prototype (Lab Tests)

1. Definition of key tasks which the users want to complete with the \SUMMA platform. 

2. Usability sessions: User observations and structured one-to-one interviews. A small number of test users will use the platform during scheduled sessions outside their operational hours for short periods of time. A member of the project team will observe the user and ask the user specific questions about the usability of the platform (e.g. are the key tasks easy to complete with this platforms?) as well as the quality of the output (e.g. accuracy of proper nouns, omission of essential words, relevance of story clustering etc.). Test users will have experience in media monitoring in the context of world news and will be fluent in one of the business critical languages: Arabic, Russian, Farsi, Ukrainian. They will trial all of the personae identified for Use Case 1. 
\medskip

The focus on particular languages depends on the component release. See section 4.1.12. The BBC will be working closely with the relevant technical partners to schedule the usability sessions accordingly.
\medskip

\textbf{Phase 2:} usability testing during the development of the improved prototype (Field Tests)
\smallskip

This test user group will include operational staff who will be asked to include the \SUMMA platform in their live operational workflow for a set period. Members of this test group will be fluent in one of the business critical languages (Arabic, Russian, Farsi, Ukrainian), and will be experts on the  media landscape of the relevant geographical region.

User participation in Phase 2 very much depends on staff availability and changing business requirements at the time. The nature of BBC Monitoring operations is defined by world news events to which they need to respond at very short notice. Therefore the user evaluation plan needs to be flexible enough to fit around the live operations with as little interference as possible.
\smallskip

1. Usage analytics will be incorporated to capture actual usage statistics. This will add automated quantitative feedback to the qualitative focus of these tests (e.g. how long did it take the user to complete their task).

2. Short questionnaires will be filled in by the test users. Structured one-to-one interviews might be carried out as well as user observations with a select number of users. 

3. Optionally, as an alternative to user observations and structured interviews, a Diary Study can be carried out: test users will be asked to keep a journal to capture their experience with the platform. The main focus of the diary analysis will be the same as in phase 1: usability of the platform and quality of the output. The journals can be captured in the form of a daily prompt by email, for example at the end of the journalists' shift, while their experience with the platform is still fresh in their memory.
		
\medskip

\textbf{Component Testing:}

\textbf{Machine Translation:}
In addition to usability testing of the platform as a whole the BBC will participate in gap filling exercises to provide human evaluation of the machine translation. More details can be found in section 5.1.2. The purpose of these exercises is to support the research efforts of the MT team (WP3) as well as evaluating the quality of the machine translation output. These exercises will require test users who are fluent in the target language (e.g. native speakers).
Specific languages will be tested as and when the relevant components have been released. See section 4.1.12: Arabic and Russian in Y2 (Q1 or Q2). Farsi and Ukrainian in Y2 (Q3). Depending on the readiness level of these components the gap filling exercises might take place nearer the end of Y2.

\textbf{Clustering and Topic Labelling}, Entity Tagging and Linking, Summarisation and Sentiment Analysis:
The components described in sections 5.1.3, 5.1.4 and 5.1.8 will also be evaluated by BBC test users. The focus will be on qualitative evaluation. Its purpose is to support the research efforts of the technical partners involved in WP3 and WP5. Three 'checkpoint' evaluation sessions are planned in total for Year 2 and Year 3; the sessions will be carried out in the lab environment and will last for approximately 4 hours each. The test language will be English. While test users carry out key tasks (see above, Phase 1) they are asked to judge the quality of clustering, topic labelling, summarisation, sentiment analysis and entity tagging. This will either be part of short questionnaires or structured one-to-one interviews. A simple rating system will be used (e.g. rate the quality on a scale from 1 to 5; or 'good', 'bad') which can be integrated into the user interface for test purposes. Field tests are desirable, but dependent on staff availability at the time.


\subsection{Internal Monitoring Use Case}

Deutsche Welle will introduce the released  platform to the different language departments and ask them to use the \SUMMA platform on a regular basis and provide feedback through the online feedback system integrated in the platform (in the form of like/dislike buttons and comment fields). This includes journalists, editors, heads of region and language groups covering the different \SUMMA languages. The users will focus on DW content provided to \SUMMA via the DW API. DW \SUMMA project managers will be in charge of coordinating evaluation sessions at Deutsche Welle. 

In addition, specialists from Deutsche Welle’s Innovation Division contribute focusing on specific innovative aspects, applications and visualisations, providing a different point of view, and will form focus test groups for consistent testing.

Evaluation in year 2 will be on the V2 prototype using the LETA Integrated Platform. Different functions will be assessed in terms of user expectations, quality and format of output, ease of use.  

This includes, for instance, feedback on the following \SUMMA output that can help the journalist/editor in his journalistic tasks:

\begin{itemize}
\item preview and quality of video
\item usefulness of the teaser
\item usefulness of the summary
\item overview through stories and storylines
\item selection of feeds
\item completeness and accuracy of named entities
\item quality of transcript in original language
\item quality of transcript in English
\item usefulness of keywords and tagging
\item monitoring enhancement through alerts
\item search facility
\item user management, preference setting
\item possibility to train the system
\item overall UI issues
\end{itemize}

In addition to such continuous feedback method, Deutsche Welle will also organise user days, workshops, etc. to engage a larger user group, both internally and within the DW network. 

For end-user sessions, user instructions, guidelines and pre-set task descriptions will be prepared to ensure smooth and clear user trials. Screencasts will be made, serving as an online demo of the tool. In a typical session, the user will be presented with the \SUMMA tool, containing a variety of functionalities, depending on the stage of development, and then asked to perform a series of pre-set tasks. A certain amount of autonomy will also be accorded to the user so that s/he can “play” with the tool and be creative in their way of using the system.

Feedback will be gathered in different ways, including:
\begin{itemize}
\item personal interviews: collecting personalised feedback during one-on-one sessions
\item questionnaires: these will be produced and made available online and are used to gather consistent user feedback – and the output can easily be analysed and visualised 
\item direct feedback on contents appearing on screen through feedback buttons and user behaviour analysis:
\begin{itemize}
\item Like/Dislike buttons
\item Keep for later button
\item Never show again button 
\end{itemize}
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.2\textwidth]{./images/like_dislike.jpg}
    \caption{Like-Dislike Buttons}
    \label{fig:like-dislike}
\end{figure}


\subsection{Data Journalism Use Case}
The third Use Case (Data Journalism) will be developed in the course of the project and detailed evaluation plans will be created in parallel, to be completed when development of the third prototype begins, i.e. by Month 24 (Milestone 11). Data journalists are likely to be using the \SUMMA platform with a different focus than monitoring journalists. Key tasks which they need to complete might be different from those established in use cases 1 and 2, and need to be identified first. Therefore it is sensible to develop the usability test plans accordingly. The user requirements for and the design of the third prototype will determine the evaluation techniques of use case 3.

